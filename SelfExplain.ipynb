{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c07e6c7a-d175-40aa-ac43-c9d24ee0dbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/s2110149/.anaconda3/envs/selfexplain/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "# Can not understand which env is being run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6383824c-8e5a-4001-9b9d-8369dc698668",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install benepar\n",
    "!benepar.download('benepar_en3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97cb98de-0760-429d-8f7b-7fb60a23ff62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'SelfExplain'...\n",
      "remote: Enumerating objects: 244, done.\u001b[K\n",
      "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
      "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
      "remote: Total 244 (delta 47), reused 41 (delta 40), pack-reused 169\u001b[K\n",
      "Receiving objects: 100% (244/244), 2.41 MiB | 21.08 MiB/s, done.\n",
      "Resolving deltas: 100% (128/128), done.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/dheerajrajagopal/SelfExplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "668aeefd-6ae7-43d3-a4cd-b32e04fa8f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/s2110149/WORKING/semantic-topic-modeling/SelfExplain'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('SelfExplain')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5887d88e-a12a-4a64-8203-8f5e467db372",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNum GPUs Available: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlist_physical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f31871-7c04-4b43-8477-8865286a204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-11 15:48:44.262108: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-11 15:48:44.880286: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-11 15:48:47.824923: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s2110149/WORKING/semantic-topic-modeling/SelfExplain/preprocessing/store_parse_trees.py\", line 58, in <module>\n",
      "    main()\n",
      "  File \"/home/s2110149/WORKING/semantic-topic-modeling/SelfExplain/preprocessing/store_parse_trees.py\", line 47, in main\n",
      "    parsed_data = ParsedDataset(tokenizer_name=args.tokenizer_name)\n",
      "  File \"/home/s2110149/WORKING/semantic-topic-modeling/SelfExplain/preprocessing/store_parse_trees.py\", line 12, in __init__\n",
      "    self.parser = ParseTree(tokenizer_name=tokenizer_name)\n",
      "  File \"/home/s2110149/WORKING/semantic-topic-modeling/SelfExplain/preprocessing/constituency_parse.py\", line 12, in __init__\n",
      "    self.parser = benepar.Parser('benepar_en3')\n",
      "  File \"/home/s2110149/.anaconda3/lib/python3.9/site-packages/benepar/integrations/nltk_plugin.py\", line 149, in __init__\n",
      "    self._parser = load_trained_model(name)\n",
      "  File \"/home/s2110149/.anaconda3/lib/python3.9/site-packages/benepar/integrations/downloader.py\", line 34, in load_trained_model\n",
      "    parser = ChartParser.from_trained(model_path)\n",
      "  File \"/home/s2110149/.anaconda3/lib/python3.9/site-packages/benepar/parse_chart.py\", line 185, in from_trained\n",
      "    parser = cls(**config)\n",
      "  File \"/home/s2110149/.anaconda3/lib/python3.9/site-packages/benepar/parse_chart.py\", line 70, in __init__\n",
      "    self.retokenizer = retokenization.Retokenizer(\n",
      "  File \"/home/s2110149/.anaconda3/lib/python3.9/site-packages/benepar/retokenization.py\", line 90, in __init__\n",
      "    self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
      "  File \"/home/s2110149/.anaconda3/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\", line 697, in from_pretrained\n",
      "    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "  File \"/home/s2110149/.anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 1804, in from_pretrained\n",
      "    return cls._from_pretrained(\n",
      "  File \"/home/s2110149/.anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 1958, in _from_pretrained\n",
      "    tokenizer = cls(*init_inputs, **init_kwargs)\n",
      "  File \"/home/s2110149/.anaconda3/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py\", line 133, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/s2110149/.anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\", line 114, in __init__\n",
      "    fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\n",
      "  File \"/home/s2110149/.anaconda3/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py\", line 1199, in convert_slow_tokenizer\n",
      "    return converter_class(transformer_tokenizer).converted()\n",
      "  File \"/home/s2110149/.anaconda3/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py\", line 438, in __init__\n",
      "    from .utils import sentencepiece_model_pb2 as model_pb2\n",
      "  File \"/home/s2110149/.anaconda3/lib/python3.9/site-packages/transformers/utils/sentencepiece_model_pb2.py\", line 91, in <module>\n",
      "    _descriptor.EnumValueDescriptor(\n",
      "  File \"/home/s2110149/.anaconda3/lib/python3.9/site-packages/google/protobuf/descriptor.py\", line 796, in __new__\n",
      "    _message.Message._CheckCalledFromGeneratedFile()\n",
      "TypeError: Descriptors cannot not be created directly.\n",
      "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
      "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
      " 1. Downgrade the protobuf package to 3.20.x or lower.\n",
      " 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
      "\n",
      "More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s2110149/WORKING/semantic-topic-modeling/SelfExplain/preprocessing/build_concept_store.py\", line 75, in <module>\n",
      "    main()\n",
      "  File \"/home/s2110149/WORKING/semantic-topic-modeling/SelfExplain/preprocessing/build_concept_store.py\", line 68, in main\n",
      "    concept_store(input_file_name=args.input_train_file,\n",
      "  File \"/home/s2110149/WORKING/semantic-topic-modeling/SelfExplain/preprocessing/build_concept_store.py\", line 24, in concept_store\n",
      "    with open(input_file_name, 'r') as input_file:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/XLNet-SUBJ/train_with_parse.json'\n"
     ]
    }
   ],
   "source": [
    "!sh scripts/run_preprocessing.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fead10-a714-4bdd-86c7-43062104e05d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96021f25-b204-46c2-b2b3-b52d4b6adac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3aad28e-f94e-4d98-8750-685c71c34ed8",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f0235b3-8435-4f11-869d-aa54a1d8b2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/s2110149/WORKING/semantic-topic-modeling/SelfExplain/model/infer_model.py\", line 12, in <module>\n",
      "    from SE_XLNet import SEXLNet\n",
      "  File \"/home/s2110149/WORKING/semantic-topic-modeling/SelfExplain/model/SE_XLNet.py\", line 5, in <module>\n",
      "    from pytorch_lightning.core.lightning import LightningModule\n",
      "ModuleNotFoundError: No module named 'pytorch_lightning.core.lightning'\n"
     ]
    }
   ],
   "source": [
    "!python model/infer_model.py \\\n",
    "    --ckpt lightning_logs/version_3/checkpoints/epoch=2-step=1499-val_acc_epoch=0.9570.ckpt \\\n",
    "      --concept_map data/RoBERTa-SST-2/concept_idx.json \\\n",
    "      --paths_output_loc result/result_roberta_7.csv \\\n",
    "      --dev_file data/RoBERTa-SST-2/dev_with_parse.json \\\n",
    "      --batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a61a463-4049-41d7-a0b5-7be71178f0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selfexplain",
   "language": "python",
   "name": "selfexplain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
